#!/usr/bin/env python3
"""
GBaseSupport Message Analyzer
ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´åˆ†æãƒ„ãƒ¼ãƒ«

Usage (CSV mode):
    python3 analyze.py --csv <path> --client <name> --period <period> [--output <dir>]

Usage (API mode):
    python3 analyze.py --dataset-id <id> --token <token> --start-date 2025-12-01 --end-date 2025-12-31 \
        --client <name> --period <period> [--output <dir>]
"""

import pandas as pd
import json
import re
import argparse
import sys
import time
import shutil
from datetime import datetime
from collections import Counter
from pathlib import Path
import html as html_module

try:
    import requests
except ImportError:
    requests = None

# ========================================
# Skill root directory (for loading templates)
# ========================================

SKILL_DIR = Path(__file__).resolve().parent.parent
TEMPLATE_DIR = SKILL_DIR / "assets"

# ========================================
# æœªå›ç­”åˆ¤å®šç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
# ========================================

UNANSWERED_KEYWORDS = [
    "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
    "æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
    "ãŠç­”ãˆã§ãã¾ã›ã‚“",
    "ä¸€è‡´ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
]

# å«è¯­ï¼ˆè‡ªå‹•å›ç­”ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰
FILLER_PHRASES = [
    "ãŠèª¿ã¹ã„ãŸã—ã¾ã™",
    "å°‘ã€…ãŠå¾…ã¡ãã ã•ã„",
    "ç¢ºèªã„ãŸã—ã¾ã™",
    "ãƒ‹ãƒ¥ã‚¦ãƒãƒ³é«˜è¼ªã®æƒ…å ±ã‚’ãŠèª¿ã¹ã„ãŸã—ã¾ã™",
]

# ========================================
# è³ªå•åˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé›¶å”®/å•†æ¥­æ–½è¨­ï¼‰
# ========================================

CATEGORY_KEYWORDS = {
    "ä½ç½®ãƒ»ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³": [
        "ã©ã“", "å ´æ‰€", "ä½ç½®", "è¡Œãæ–¹", "ã‚¢ã‚¯ã‚»ã‚¹", "ä½•éš", "ãƒ•ãƒ­ã‚¢", "ãƒãƒƒãƒ—", "æ¡ˆå†…",
        "LUFTBAUM", "ãƒ«ãƒ•ãƒˆãƒã‚¦ãƒ ", "28F", "29F", "å±‹ä¸Š", "å—é¤¨", "åŒ—é¤¨", "South", "North",
        "ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼", "ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚¿ãƒ¼", "éšæ®µ", "å…¥å£", "å‡ºå£",
    ],
    "åº—èˆ—ãƒ»å•†å“ç…§ä¼š": [
        "åº—èˆ—", "ã‚·ãƒ§ãƒƒãƒ—", "åº—", "ãƒ–ãƒ©ãƒ³ãƒ‰", "ã‚«ãƒ•ã‚§", "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³", "é£Ÿã¹ç‰©", "é£²ã¿ç‰©",
        "æœ", "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "é›‘è²¨", "ã‚³ã‚¹ãƒ¡", "ç¾å®¹", "ã‚¯ãƒªãƒ‹ãƒƒã‚¯",
        "ãƒ¡ãƒ‹ãƒ¥ãƒ¼", "å•†å“", "åœ¨åº«", "ä¾¡æ ¼", "æ–™é‡‘",
        "ã‚ã‚“ã“", "çŠ¬", "ãƒšãƒƒãƒˆ", "ãƒªãƒ¼ãƒ‰",
        "å’Œè“å­", "ã‚¹ã‚¤ãƒ¼ãƒ„", "ãƒ‘ãƒ³", "æœ¬", "æœ¬å±‹",
    ],
    "æ–½è¨­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹": [
        "é§è»Šå ´", "é§è¼ªå ´", "ATM", "ãƒˆã‚¤ãƒ¬", "ãŠæ‰‹æ´—ã„", "åŒ–ç²§å®¤", "æˆä¹³å®¤", "ãƒ™ãƒ“ãƒ¼ãƒ«ãƒ¼ãƒ ",
        "ã‚³ã‚¤ãƒ³ãƒ­ãƒƒã‚«ãƒ¼", "è·ç‰©", "é ã‹ã‚Š", "Wi-Fi", "wifi", "å……é›»", "ã‚³ãƒ³ã‚»ãƒ³ãƒˆ",
        "ãƒ™ãƒ“ãƒ¼ã‚«ãƒ¼", "è»Šæ¤…å­", "ãƒ¬ãƒ³ã‚¿ãƒ«", "è²¸å‡º",
    ],
    "å–¶æ¥­æ™‚é–“": [
        "å–¶æ¥­æ™‚é–“", "ä½•æ™‚ã‹ã‚‰", "ä½•æ™‚ã¾ã§", "é–‹åº—", "é–‰åº—", "ä¼‘æ¥­æ—¥", "å®šä¼‘æ—¥",
        "å¹´æœ«å¹´å§‹", "ç¥æ—¥", "ã‚¤ãƒ™ãƒ³ãƒˆ",
    ],
    "ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³": [
        "ã‚¤ãƒ™ãƒ³ãƒˆ", "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³", "ã‚»ãƒ¼ãƒ«", "å‰²å¼•", "ãƒã‚¤ãƒ³ãƒˆ", "ä¼šå“¡", "ç‰¹å…¸",
        "å±•ç¤ºä¼š", "æœŸé–“é™å®š", "ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—",
    ],
    "ã‚¯ãƒ¬ãƒ¼ãƒ ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯": [
        "æœ€æ‚ª", "ã²ã©ã„", "ä¸æº€", "ã‚¯ãƒ¬ãƒ¼ãƒ ", "æ”¹å–„", "è¦æœ›", "ã‚ã‚ŠãŒã¨ã†", "ãŠä¸–è©±",
        "åŠ©ã‹ã£ãŸ", "ä¾¿åˆ©", "å¬‰ã—ã„",
    ],
}


def detect_language(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¨€èªã‚’æ¤œå‡º"""
    if not isinstance(text, str):
        return "ãã®ä»–"

    if re.search(r'[\uAC00-\uD7A3]', text):
        return "éŸ“å›½èª"
    if re.search(r'[\u3040-\u309F\u30A0-\u30FF]', text):
        return "æ—¥æœ¬èª"
    if re.search(r'[\u4E00-\u9FFF]', text) and any(c in text for c in 'çš„æ˜¯åœ¨ä¸€ä¸äº†æœ‰å’Œä¸ªäººæˆ‘è¿™ä¸Šä¸ªä¸ºåˆ°'):
        return "ä¸­å›½èª"
    if re.match(r'^[a-zA-Z\s\d\W]+$', text) and len(re.findall(r'[a-zA-Z]{2,}', text)) > 2:
        return "è‹±èª"

    return "ãã®ä»–"


def is_unanswered(answer):
    """æœªå›ç­”ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if pd.isna(answer) or not isinstance(answer, str):
        return True, "æƒ…å ±ãªã—"

    answer_stripped = answer.strip()
    if not answer_stripped:
        return True, "æƒ…å ±ãªã—"

    for keyword in UNANSWERED_KEYWORDS:
        if keyword in answer:
            return True, "æƒ…å ±ãªã—"

    cleaned = answer_stripped
    for filler in FILLER_PHRASES:
        cleaned = cleaned.replace(filler, "")
    cleaned = cleaned.strip()

    if len(cleaned) < 20:
        if "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“" in answer or "ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸ" in answer:
            return True, "æƒ…å ±ãªã—"
        return True, "å†ç¢ºèª"

    return False, None


def classify_question(question):
    """è³ªå•ã‚’åˆ†é¡"""
    if pd.isna(question) or not isinstance(question, str):
        return "é›‘è«‡ãƒ»ãã®ä»–"

    question_lower = question.lower()
    for category, keywords in CATEGORY_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in question_lower:
                return category

    return "é›‘è«‡ãƒ»ãã®ä»–"


def categorize_unanswered(question, answer):
    """æœªå›ç­”ã®ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
    if pd.isna(answer) or not isinstance(answer, str):
        return "æƒ…å ±ãªã—"

    for kw in UNANSWERED_KEYWORDS:
        if kw in answer:
            return "æƒ…å ±ãªã—"

    if any(p in answer for p in ["è©²å½“ã™ã‚‹", "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "ä¸€è‡´ã—ã¾ã›ã‚“"]):
        return "æ¤œç´¢å¤±æ•—"

    return "å†ç¢ºèª"


def count_session_depth(df):
    """ä¼šè©±æ·±åº¦ã‚’é›†è¨ˆ"""
    session_counts = df.groupby('ãƒãƒ£ãƒƒãƒˆ ID').size()
    depth = {"1å›": 0, "2å›": 0, "3å›": 0, "4å›+": 0}
    for count in session_counts:
        if count == 1:
            depth["1å›"] += 1
        elif count == 2:
            depth["2å›"] += 1
        elif count == 3:
            depth["3å›"] += 1
        else:
            depth["4å›+"] += 1
    return depth


def detect_media_type(answer):
    """å›ç­”ã«å«ã¾ã‚Œã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡º"""
    if pd.isna(answer) or not isinstance(answer, str):
        return "ãƒ†ã‚­ã‚¹ãƒˆã®ã¿"

    has_link = bool(re.search(r'https?://', answer))
    has_image = bool(re.search(r'!\[.*?\]\(.*?\)|<img|src=', answer))
    has_table = bool(re.search(r'<table|---------|\|.*\|', answer))

    if has_image:
        return "ç”»åƒå«ã‚€"
    elif has_table:
        return "ãƒ†ãƒ¼ãƒ–ãƒ«å«ã‚€"
    elif has_link:
        return "ãƒªãƒ³ã‚¯å«ã‚€"
    else:
        return "ãƒ†ã‚­ã‚¹ãƒˆã®ã¿"


def escape_html(text):
    """HTMLç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—"""
    if pd.isna(text):
        return ""
    return html_module.escape(str(text))


# ========================================
# GBase API ãƒ‡ãƒ¼ã‚¿å–å¾—
# ========================================

def _api_headers(token):
    """API ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    return {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
    }


def resolve_ai_id(base_url, token, dataset_id):
    """dataset_id ã‹ã‚‰ ai_id ã‚’é€†å¼•ãã™ã‚‹

    æ–¹æ³•1: GET /datasets/{dataset_id} â†’ robots ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ç›´æ¥å–å¾—ï¼ˆé«˜é€Ÿï¼‰
    æ–¹æ³•2: GET /robots ä¸€è¦§ã‹ã‚‰ dataset_id ã‚’æ¢ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    """
    if requests is None:
        print("âŒ requests ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™: pip install requests")
        sys.exit(1)

    headers = _api_headers(token)

    # --- æ–¹æ³•1: GET /datasets/{dataset_id} ã‹ã‚‰ robots ã‚’ç›´æ¥å–å¾— ---
    try:
        resp = requests.get(
            f"{base_url}/datasets/{dataset_id}",
            headers=headers,
            timeout=30,
        )
        if resp.status_code == 200:
            ds_data = resp.json()
            robots = ds_data.get("robots", [])
            if robots:
                robot = robots[0]
                robot_id = robot.get("id", "") if isinstance(robot, dict) else str(robot)
                robot_name = robot.get("name", "") if isinstance(robot, dict) else ""
                if robot_id:
                    print(f"âœ… dataset_id â†’ ai_id è§£æ±º: {robot_id} ({robot_name})")
                    return str(robot_id)
    except Exception:
        pass  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¸

    # --- æ–¹æ³•2: GET /robots ä¸€è¦§ã‹ã‚‰æ¢ã™ ---
    page = 1
    size = 200

    while True:
        resp = requests.get(
            f"{base_url}/robots",
            headers=headers,
            params={"page": page, "size": size},
            timeout=30,
        )
        resp.raise_for_status()
        data = resp.json()

        items = data.get("items", [])
        if not items:
            break

        for robot in items:
            robot_id = robot.get("id")
            if not robot_id:
                continue

            datasets = robot.get("datasets", [])
            for ds in datasets:
                ds_id = ds.get("id", "") if isinstance(ds, dict) else str(ds)
                if str(ds_id) == str(dataset_id):
                    print(f"âœ… dataset_id â†’ ai_id è§£æ±º: {robot_id} ({robot.get('name', '')})")
                    return str(robot_id)

            if str(robot.get("default_dataset_id", "")) == str(dataset_id):
                print(f"âœ… dataset_id â†’ ai_id è§£æ±º: {robot_id} ({robot.get('name', '')})")
                return str(robot_id)

        total_pages = data.get("pages", 1)
        if page >= total_pages:
            break
        page += 1

    print(f"âŒ dataset_id '{dataset_id}' ã«å¯¾å¿œã™ã‚‹AIãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    sys.exit(1)


def fetch_from_api(base_url, token, dataset_id, start_date, end_date, ai_id=None):
    """GBase API ã‹ã‚‰æ¶ˆæ¯å±¥æ­´ã‚’å–å¾—ã—ã€CSV äº’æ›ã® DataFrame ã‚’è¿”ã™

    1. dataset_id â†’ ai_id ã®è§£æ±ºï¼ˆai_id ãŒç›´æ¥æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    2. /questions/{ai_id}/session.messages.history.list ã§å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†é¡µå–å¾—
    3. API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ CSV ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã® DataFrame ã«å¤‰æ›
    """
    if requests is None:
        print("âŒ requests ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™: pip install requests")
        sys.exit(1)

    # Step 1: ai_id ã‚’è§£æ±º
    if ai_id:
        print(f"âœ… ai_id ç›´æ¥æŒ‡å®š: {ai_id}")
    elif dataset_id:
        ai_id = resolve_ai_id(base_url, token, dataset_id)
    else:
        print("âŒ --dataset-id ã¾ãŸã¯ --ai-id ãŒå¿…è¦ã§ã™")
        sys.exit(1)

    # Step 2: æ™‚é–“ç¯„å›²ã‚’ISO 8601å½¢å¼ã«å¤‰æ›
    start_time = f"{start_date}T00:00:00Z"
    end_time = f"{end_date}T23:59:59Z"

    headers = _api_headers(token)
    all_messages = []
    page = 1
    size = 1000  # APIæœ€å¤§å€¤

    print(f"ğŸ“¡ API ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’å–å¾—ä¸­...")
    print(f"   AI ID: {ai_id}")
    print(f"   æœŸé–“: {start_date} ~ {end_date}")

    while True:
        resp = requests.post(
            f"{base_url}/questions/{ai_id}/session.messages.history.list",
            headers=headers,
            params={
                "start_time": start_time,
                "end_time": end_time,
                "page": page,
                "size": size,
                "include_test": "false",
            },
            timeout=60,
        )
        resp.raise_for_status()
        data = resp.json()

        items = data.get("items", [])
        all_messages.extend(items)

        total = data.get("total", 0)
        total_pages = data.get("pages", 1)
        print(f"   ãƒšãƒ¼ã‚¸ {page}/{total_pages} å–å¾—å®Œäº†ï¼ˆç´¯è¨ˆ: {len(all_messages)}/{total}ä»¶ï¼‰")

        if page >= total_pages:
            break
        page += 1
        time.sleep(0.3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

    if not all_messages:
        print("âŒ æŒ‡å®šæœŸé–“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    print(f"âœ… åˆè¨ˆ {len(all_messages)} ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—")

    # Step 3: API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ â†’ CSVäº’æ› DataFrame ã«å¤‰æ›
    rows = []
    for msg in all_messages:
        # feedback_type/rating â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        feedback_type = msg.get("feedback_type")
        rating = msg.get("rating", 0)
        if feedback_type == "good" or (isinstance(rating, int) and rating > 0):
            feedback = "è‰¯ã„"
        elif feedback_type == "bad" or (isinstance(rating, int) and rating < 0):
            feedback = "æ‚ªã„"
        else:
            feedback = "-"

        # transfer_to_human â†’ æ‹…å½“è€…ã«æ¥ç¶šæ¸ˆã¿
        transfer = "ã¯ã„" if msg.get("transfer_to_human") else "ã„ã„ãˆ"

        rows.append({
            "è³ªå•æ™‚é–“": msg.get("created_at", ""),
            "è³ªå•": msg.get("question", ""),
            "å›ç­”": msg.get("answer", ""),
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯": feedback,
            "è©•ä¾¡ç†ç”±": msg.get("feedback_content", ""),
            "ãƒãƒ£ãƒƒãƒˆ ID": msg.get("session_id", ""),
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼": msg.get("user_id", ""),
            "æ‹…å½“è€…ã«æ¥ç¶šæ¸ˆã¿": transfer,
        })

    df = pd.DataFrame(rows)
    return df


def compute_kpi_for_dashboard(df, kpi, period, year_month, avg_daily, language_counts, feedback_rate):
    """Dashboardç”¨ã®KPIçµ±è¨ˆã‚’è¨ˆç®—"""
    total = kpi["total_messages"]
    foreign_count = sum(int(language_counts.get(l, 0)) for l in ["è‹±èª", "ä¸­å›½èª", "éŸ“å›½èª"])
    foreign_pct = foreign_count / total * 100 if total > 0 else 0

    # æœ‰äººå¯¾å¿œç‡
    if 'æ‹…å½“è€…ã«æ¥ç¶šæ¸ˆã¿' in df.columns:
        human_count = len(df[df['æ‹…å½“è€…ã«æ¥ç¶šæ¸ˆã¿'] == 'ã¯ã„'])
        human_rate = human_count / total * 100 if total > 0 else 0
    else:
        human_rate = 0

    unique_users = df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'].nunique() if 'ãƒ¦ãƒ¼ã‚¶ãƒ¼' in df.columns else 0

    # æ›œæ—¥åˆ¥åˆ†å¸ƒ (æœˆ~æ—¥ = 0~6)
    weekday_counts = df['æ›œæ—¥'].value_counts().sort_index()
    weekday_data = [int(weekday_counts.get(i, 0)) for i in range(7)]

    return {
        "period": period,
        "year_month": year_month,
        "total_messages": total,
        "normal_answer_rate": round(kpi["normal_answer_rate"], 1),
        "unanswered_rate": round(kpi["unanswered_rate"], 1),
        "good_rating_rate": round(kpi["good_rate"], 1),
        "feedback_rate": round(feedback_rate, 1),
        "daily_average": round(avg_daily, 1),
        "unique_users": unique_users,
        "human_transfer_rate": round(human_rate, 1),
        "foreign_language_pct": round(foreign_pct, 1),
        "weekday_counts": weekday_data,
    }


def update_dashboard_json(site_dir, client_slug, client_name, month_stats,
                          report_filename, unanswered_filename=None):
    """Dashboard JSONã‚’æ›´æ–°ã—ã€ãƒ¬ãƒãƒ¼ãƒˆHTMLã‚’ã‚µã‚¤ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼"""
    client_dir = Path(site_dir) / "clients" / client_slug
    reports_dir = client_dir / "reports"
    reports_dir.mkdir(parents=True, exist_ok=True)

    json_path = client_dir / "dashboard-data.json"

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    if json_path.exists():
        with open(json_path, "r", encoding="utf-8") as f:
            dashboard = json.load(f)
    else:
        dashboard = {
            "client": client_name,
            "client_slug": client_slug,
            "updated_at": "",
            "months": [],
        }

    # åŒã˜ year_month ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ç½®æ›ã€ãªã‘ã‚Œã°è¿½åŠ 
    year_month = month_stats["year_month"]
    months = [m for m in dashboard["months"] if m["year_month"] != year_month]

    # report_file / unanswered_file ãƒ‘ã‚¹ã‚’è¿½åŠ 
    month_entry = dict(month_stats)
    month_entry["report_file"] = f"reports/{year_month}.html"
    if unanswered_filename:
        month_entry["unanswered_file"] = f"reports/{year_month}_unanswered.html"

    months.append(month_entry)
    months.sort(key=lambda m: m["year_month"])

    dashboard["months"] = months
    dashboard["updated_at"] = datetime.now().isoformat()
    dashboard["client"] = client_name
    dashboard["client_slug"] = client_slug

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(dashboard, f, ensure_ascii=False, indent=2)

    print(f"âœ… dashboard-data.json æ›´æ–°: {json_path}")
    return json_path


def main():
    parser = argparse.ArgumentParser(
        description='GBaseSupport Message Analyzer',
        epilog='Either --csv (CSV mode) or --dataset-id + --token (API mode) is required.',
    )
    # Common arguments
    parser.add_argument('--client', required=True, help='Client name (e.g. NEWoMané«˜è¼ª)')
    parser.add_argument('--period', required=True, help='Report period (e.g. 2025å¹´12æœˆ)')
    parser.add_argument('--output', default=None, help='Output directory (default: same as CSV or current dir)')

    # CSV mode
    parser.add_argument('--csv', default=None, help='Path to chat history CSV file (CSV mode)')

    # API mode
    parser.add_argument('--dataset-id', default=None, help='GBase dataset ID (API mode)')
    parser.add_argument('--ai-id', default=None, help='GBase AI/robot ID - skip dataset_id lookup (API mode)')
    parser.add_argument('--token', default=None, help='GBase API bearer token (API mode)')
    parser.add_argument('--api-url', default='https://api.gbase.ai', help='GBase API base URL (default: https://api.gbase.ai)')
    parser.add_argument('--start-date', default=None, help='Start date YYYY-MM-DD (API mode)')
    parser.add_argument('--end-date', default=None, help='End date YYYY-MM-DD (API mode)')

    # Site integration mode
    parser.add_argument('--site-dir', default=None, help='Path to docs/ directory for dashboard site integration')
    parser.add_argument('--client-slug', default=None, help='URL-safe client identifier (e.g. newoman-takanawa)')

    args = parser.parse_args()

    client_name = args.client
    period = args.period

    # Determine data source mode
    if args.csv:
        # === CSV Mode ===
        csv_path = Path(args.csv).resolve()
        output_dir = Path(args.output).resolve() if args.output else csv_path.parent

        print(f"ğŸ” åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆCSVãƒ¢ãƒ¼ãƒ‰ï¼‰: {csv_path}")

        for encoding in ['utf-8', 'utf-8-sig', 'shift-jis', 'cp932']:
            try:
                df = pd.read_csv(csv_path, encoding=encoding)
                print(f"âœ… CSVèª­ã¿è¾¼ã¿æˆåŠŸï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding}ï¼‰")
                break
            except Exception:
                continue
        else:
            print("âŒ CSVèª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)

    elif (args.dataset_id or args.ai_id) and args.token:
        # === API Mode ===
        if not args.start_date or not args.end_date:
            parser.error('API mode requires --start-date and --end-date')

        output_dir = Path(args.output).resolve() if args.output else Path.cwd()

        print(f"ğŸ” åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆAPIãƒ¢ãƒ¼ãƒ‰ï¼‰")
        df = fetch_from_api(args.api_url, args.token, args.dataset_id, args.start_date, args.end_date, ai_id=args.ai_id)

    else:
        parser.error('Either --csv or (--dataset-id/--ai-id + --token + --start-date + --end-date) is required')

    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")

    # å‰å‡¦ç†
    df['è³ªå•æ™‚é–“'] = pd.to_datetime(df['è³ªå•æ™‚é–“'], errors='coerce')
    df = df.dropna(subset=['è³ªå•æ™‚é–“'])

    # APIãƒ‡ãƒ¼ã‚¿ã¯UTC (+00:00) â†’ JST (+09:00) ã«å¤‰æ›
    if df['è³ªå•æ™‚é–“'].dt.tz is not None:
        df['è³ªå•æ™‚é–“'] = df['è³ªå•æ™‚é–“'].dt.tz_convert('Asia/Tokyo').dt.tz_localize(None)
        print("ğŸ• ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å¤‰æ›: UTC â†’ JST (+9h)")

    df['æ—¥ä»˜'] = df['è³ªå•æ™‚é–“'].dt.date
    df['æ›œæ—¥'] = df['è³ªå•æ™‚é–“'].dt.dayofweek
    df['æ™‚é–“'] = df['è³ªå•æ™‚é–“'].dt.hour

    # KPIè¨ˆç®—
    total_messages = len(df)

    unanswered_results = df['å›ç­”'].apply(is_unanswered)
    df['æœªå›ç­”ãƒ•ãƒ©ã‚°'] = unanswered_results.apply(lambda x: x[0])
    df['æœªå›ç­”ã‚¿ã‚¤ãƒ—'] = unanswered_results.apply(lambda x: x[1] if x[0] else None)

    unanswered_count = df['æœªå›ç­”ãƒ•ãƒ©ã‚°'].sum()
    answered_count = total_messages - unanswered_count

    feedback_df = df[df['ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯'].notna() & (df['ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯'] != '-')]
    good_feedback = len(feedback_df[feedback_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯'].isin(['è‰¯ã„', 'è‰¯ã„è©•ä¾¡'])])
    bad_feedback = len(feedback_df[feedback_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯'].isin(['æ‚ªã„', 'æ‚ªã„è©•ä¾¡'])])

    feedback_count = good_feedback + bad_feedback
    feedback_rate = feedback_count / total_messages * 100 if total_messages > 0 else 0
    good_rate = good_feedback / feedback_count * 100 if feedback_count > 0 else 0
    normal_answer_rate = answered_count / total_messages * 100 if total_messages > 0 else 0
    unanswered_rate = unanswered_count / total_messages * 100 if total_messages > 0 else 0

    kpi = {
        "total_messages": total_messages,
        "answered_count": answered_count,
        "unanswered_count": int(unanswered_count),
        "good_feedback": good_feedback,
        "bad_feedback": bad_feedback,
        "feedback_count": feedback_count,
        "normal_answer_rate": normal_answer_rate,
        "unanswered_rate": unanswered_rate,
        "good_rate": good_rate,
        "feedback_rate": feedback_rate,
    }

    print(f"\nğŸ“ˆ KPIã‚µãƒãƒªãƒ¼:")
    print(f"  ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {kpi['total_messages']}")
    print(f"  æ­£å¸¸å›ç­”ç‡: {kpi['normal_answer_rate']:.1f}%")
    print(f"  æœªå›ç­”ç‡: {kpi['unanswered_rate']:.1f}%")
    print(f"  å¥½è©•ä¾¡ç‡: {kpi['good_rate']:.1f}%")

    # æ™‚é–“åˆ†æ
    daily_counts = df.groupby(df['è³ªå•æ™‚é–“'].dt.day).size()
    max_day_in_data = int(df['è³ªå•æ™‚é–“'].dt.day.max())
    min_day_in_data = int(df['è³ªå•æ™‚é–“'].dt.day.min())
    daily_data = [int(daily_counts.get(i, 0)) for i in range(min_day_in_data, max_day_in_data + 1)]

    year = int(df['è³ªå•æ™‚é–“'].dt.year.iloc[0])
    month = int(df['è³ªå•æ™‚é–“'].dt.month.iloc[0])
    month_str = str(month)
    weekday_names_ja = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
    daily_labels = [f"{month_str}/{i}({weekday_names_ja[datetime(year, month, i).weekday()]})"
                    for i in range(min_day_in_data, max_day_in_data + 1)]

    weekday_counts = df['æ›œæ—¥'].value_counts().sort_index()
    weekday_data = [int(weekday_counts.get(i, 0)) for i in range(7)]

    hour_ranges = [(0, 6), (6, 9), (9, 12), (12, 15), (15, 18), (18, 21), (21, 24)]
    hour_data = [int(len(df[(df['æ™‚é–“'] >= s) & (df['æ™‚é–“'] < e)])) for s, e in hour_ranges]

    num_days = len(daily_counts)
    avg_daily = float(total_messages) / num_days if num_days > 0 else 0
    max_day = int(daily_counts.idxmax())
    max_count = int(daily_counts.max())
    min_day = int(daily_counts.idxmin())
    min_count = int(daily_counts.min())

    # è³ªå•åˆ†é¡
    df['ã‚«ãƒ†ã‚´ãƒª'] = df['è³ªå•'].apply(classify_question)
    category_counts = df['ã‚«ãƒ†ã‚´ãƒª'].value_counts()

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•
    session_depth = count_session_depth(df)
    df['è¨€èª'] = df['è³ªå•'].apply(detect_language)
    language_counts = df['è¨€èª'].value_counts()

    # ãƒ¡ãƒ‡ã‚£ã‚¢
    df['ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—'] = df['å›ç­”'].apply(detect_media_type)
    media_counts = df['ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—'].value_counts()

    # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
    unanswered_df = df[df['æœªå›ç­”ãƒ•ãƒ©ã‚°'] == True].copy()
    if len(unanswered_df) > 0:
        unanswered_df['ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—'] = unanswered_df.apply(
            lambda row: categorize_unanswered(row['è³ªå•'], row['å›ç­”']), axis=1)
    else:
        unanswered_df['ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—'] = []

    error_counts = unanswered_df['ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—'].value_counts()
    error_stats = {
        "info_nashi": int(error_counts.get("æƒ…å ±ãªã—", 0)),
        "search_fail": int(error_counts.get("æ¤œç´¢å¤±æ•—", 0)),
        "reconfirm": int(error_counts.get("å†ç¢ºèª", 0)),
    }

    # æœªå›ç­”ä¸€è¦§ãƒ‡ãƒ¼ã‚¿
    unanswered_list = []
    for idx, row in unanswered_df.iterrows():
        error_type = row.get('ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—', 'æƒ…å ±ãªã—')
        badge_class = {'æƒ…å ±ãªã—': 'badge-red', 'æ¤œç´¢å¤±æ•—': 'badge-yellow', 'å†ç¢ºèª': 'badge-gray'}.get(error_type, 'badge-gray')
        unanswered_list.append({
            'timestamp': row['è³ªå•æ™‚é–“'].strftime('%Y-%m-%d %H:%M:%S'),
            'error_type': error_type,
            'badge_class': badge_class,
            'question': escape_html(str(row['è³ªå•'])[:100] + ('...' if len(str(row['è³ªå•'])) > 100 else '')),
            'answer': escape_html(str(row['å›ç­”'])[:150] + ('...' if len(str(row['å›ç­”'])) > 150 else '')),
        })

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
    with open(TEMPLATE_DIR / "report-template.html", "r", encoding="utf-8") as f:
        main_template = f.read()
    with open(TEMPLATE_DIR / "unanswered-template.html", "r", encoding="utf-8") as f:
        unanswered_template = f.read()

    # ========================================
    # ãƒ¡ã‚¤ãƒ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç½®æ›ï¼‰
    # ========================================

    generated_date = datetime.now().strftime("%Y/%m/%d")

    category_order = ["ä½ç½®ãƒ»ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³", "é›‘è«‡ãƒ»ãã®ä»–", "åº—èˆ—ãƒ»å•†å“ç…§ä¼š", "æ–½è¨­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹", "å–¶æ¥­æ™‚é–“"]
    badge_colors = {
        "ä½ç½®ãƒ»ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³": "badge-blue",
        "é›‘è«‡ãƒ»ãã®ä»–": "badge-gray",
        "åº—èˆ—ãƒ»å•†å“ç…§ä¼š": "badge-green",
        "æ–½è¨­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹": "badge-yellow",
        "å–¶æ¥­æ™‚é–“": "badge-purple",
    }
    category_data = []
    for cat in category_order:
        count = int(category_counts.get(cat, 0))
        pct = count / total_messages * 100 if total_messages > 0 else 0
        category_data.append({"name": cat, "count": count, "percent": pct, "badge": badge_colors.get(cat, "badge-blue")})

    main_html = main_template
    main_html = main_html.replace("NEWoMan é«˜è¼ª", client_name)
    main_html = main_html.replace("NEWoMané«˜è¼ª 2025å¹´12æœˆ", f"{client_name} {period}")
    main_html = main_html.replace("2025/01/14", generated_date)

    # åˆ†ææœŸé–“ã‚’å‹•çš„ã«è¨­å®šï¼ˆãƒ‡ãƒ¼ã‚¿ã®å®Ÿéš›ã®æ—¥ä»˜ç¯„å›²ï¼‰
    date_from = df['è³ªå•æ™‚é–“'].min().strftime('%Y/%m/%d')
    date_to = df['è³ªå•æ™‚é–“'].max().strftime('%Y/%m/%d')
    main_html = main_html.replace("2025/12/01 - 2025/12/31", f"{date_from} - {date_to}")

    unanswered_filename = f"{client_name}_{period}_æœªå›ç­”ä¸€è¦§.html"
    main_html = main_html.replace("__UNANSWERED_LINK__", unanswered_filename)

    # KPI replacements
    main_html = re.sub(r'<div class="kpi-value">909</div>', f'<div class="kpi-value">{kpi["total_messages"]}</div>', main_html)
    main_html = re.sub(r'<div class="kpi-value">89\.1%</div>', f'<div class="kpi-value">{kpi["normal_answer_rate"]:.1f}%</div>', main_html)
    main_html = re.sub(r'<div class="kpi-sublabel">810/909</div>', f'<div class="kpi-sublabel">{kpi["answered_count"]}/{kpi["total_messages"]}</div>', main_html)
    main_html = re.sub(r'<div class="kpi-value">10\.9%</div>', f'<div class="kpi-value">{kpi["unanswered_rate"]:.1f}%</div>', main_html)
    main_html = re.sub(r'<div class="kpi-sublabel">99/909</div>', f'<div class="kpi-sublabel">{kpi["unanswered_count"]}/{kpi["total_messages"]}</div>', main_html)
    main_html = re.sub(r'<div class="kpi-value">70\.0%</div>', f'<div class="kpi-value">{kpi["good_rate"]:.1f}%</div>', main_html)
    main_html = re.sub(r'<div class="kpi-value">2\.2%</div>', f'<div class="kpi-value">{kpi["feedback_rate"]:.1f}%</div>', main_html)
    main_html = re.sub(r'<div class="kpi-sublabel">20/909</div>', f'<div class="kpi-sublabel">è©•ä¾¡ã‚ã‚Š {kpi["feedback_count"]}ä»¶ä¸­</div>', main_html)

    # Stats
    main_html = re.sub(r'<div class="stat-value">29\.3</div>', f'<div class="stat-value">{avg_daily:.1f}</div>', main_html)
    main_html = re.sub(r'<div class="stat-value">84</div>', f'<div class="stat-value">{max_count}</div>', main_html)
    main_html = re.sub(r'<div class="stat-value">6</div>', f'<div class="stat-value">{min_count}</div>', main_html)
    max_date_label = f"{month}/{max_day}"
    min_date_label = f"{month}/{min_day}"
    main_html = main_html.replace("__MAX_DATE__", max_date_label)
    main_html = main_html.replace("__MIN_DATE__", min_date_label)

    # Chart data
    main_html = re.sub(r"data: \[19,24,33,33,21,47,44,22,17,19,30,34,37,65,33,24,36,22,84,42,45,20,30,8,9,6,17,27,27,13,21\]", f"data: {daily_data}", main_html)
    main_html = re.sub(r"__DAILY_LABELS__", f"{daily_labels}", main_html)
    main_html = re.sub(r"data: \[121, 108, 117, 94, 145, 143, 181\]", f"data: {weekday_data}", main_html)
    main_html = re.sub(r"data: \[4, 4, 151, 344, 259, 129, 18\]", f"data: {hour_data}", main_html)

    # Category
    cat_values = [d["count"] for d in category_data]
    main_html = re.sub(r"data: \[302, 256, 233, 97, 11\]", f"data: {cat_values}", main_html)

    table_rows = ""
    for cat in category_data:
        table_rows += f'''
                            <tr>
                                <td><span class="badge {cat['badge']}">{cat['name']}</span></td>
                                <td class="number">{cat['count']}</td>
                                <td class="number">{cat['percent']:.1f}%</td>
                            </tr>'''
    main_html = re.sub(r'<tbody>.*?</tbody>', f'<tbody>{table_rows}</tbody>', main_html, flags=re.DOTALL)

    # Session depth
    depth_data = [int(session_depth[k]) for k in ["1å›", "2å›", "3å›", "4å›+"]]
    main_html = re.sub(r"data: \[376, 117, 35, 30\]", f"data: {depth_data}", main_html)

    # Language
    lang_values = [int(language_counts.get(l, 0)) for l in ["æ—¥æœ¬èª", "è‹±èª", "ä¸­å›½èª", "éŸ“å›½èª"]]
    main_html = re.sub(r"data: \[811, 55, 32, 11\]", f"data: {lang_values}", main_html)

    # Media
    media_values = [int(media_counts.get(m, 0)) for m in ["ãƒªãƒ³ã‚¯å«ã‚€", "ç”»åƒå«ã‚€", "ãƒ†ãƒ¼ãƒ–ãƒ«å«ã‚€", "ãƒ†ã‚­ã‚¹ãƒˆã®ã¿"]]
    main_html = re.sub(r"data: \[269, 83, 179, 639\]", f"data: {media_values}", main_html)

    # Unanswered CTA
    main_html = re.sub(r'<div class="cta-value">104</div>', f'<div class="cta-value">{kpi["unanswered_count"]}</div>', main_html)
    main_html = re.sub(r'<span><span class="badge badge-red">æƒ…å ±ãªã—</span> 83ä»¶</span>', f'<span><span class="badge badge-red">æƒ…å ±ãªã—</span> {error_stats["info_nashi"]}ä»¶</span>', main_html)
    main_html = re.sub(r'<span><span class="badge badge-yellow">æ¤œç´¢å¤±æ•—</span> 20ä»¶</span>', f'<span><span class="badge badge-yellow">æ¤œç´¢å¤±æ•—</span> {error_stats["search_fail"]}ä»¶</span>', main_html)
    main_html = re.sub(r'<span><span class="badge badge-gray">å†ç¢ºèª</span> 1ä»¶</span>', f'<span><span class="badge badge-gray">å†ç¢ºèª</span> {error_stats["reconfirm"]}ä»¶</span>', main_html)

    # Error cards
    uc = kpi["unanswered_count"]
    main_html = re.sub(r'<div class="error-value">83</div>', f'<div class="error-value">{error_stats["info_nashi"]}</div>', main_html)
    main_html = re.sub(r'<div class="error-percent">80%</div>', f'<div class="error-percent">{error_stats["info_nashi"]/uc*100:.0f}%</div>' if uc > 0 else '<div class="error-percent">0%</div>', main_html)
    main_html = re.sub(r'<div class="error-value">20</div>', f'<div class="error-value">{error_stats["search_fail"]}</div>', main_html)
    main_html = re.sub(r'<div class="error-percent">19%</div>', f'<div class="error-percent">{error_stats["search_fail"]/uc*100:.0f}%</div>' if uc > 0 else '<div class="error-percent">0%</div>', main_html)
    main_html = re.sub(r'<div class="error-value">1</div>', f'<div class="error-value">{error_stats["reconfirm"]}</div>', main_html)
    main_html = re.sub(r'<div class="error-percent">1%</div>', f'<div class="error-percent">{error_stats["reconfirm"]/uc*100:.0f}%</div>' if uc > 0 else '<div class="error-percent">0%</div>', main_html)

    # ========================================
    # åˆ†æã‚µãƒãƒªãƒ¼ï¼ˆå‹•çš„ç”Ÿæˆï¼‰
    # ========================================

    # åˆ©ç”¨ãƒ”ãƒ¼ã‚¯æ™‚é–“å¸¯
    peak_hour_idx = hour_data.index(max(hour_data))
    hour_range_labels = ["0ã€œ6æ™‚", "6ã€œ9æ™‚", "9ã€œ12æ™‚", "12ã€œ15æ™‚", "15ã€œ18æ™‚", "18ã€œ21æ™‚", "21ã€œ24æ™‚"]
    peak_hour_label = hour_range_labels[peak_hour_idx]
    peak_hour_pct = max(hour_data) / total_messages * 100 if total_messages > 0 else 0

    # æœ€å¤šæ›œæ—¥
    weekday_names_full = ['æœˆæ›œæ—¥', 'ç«æ›œæ—¥', 'æ°´æ›œæ—¥', 'æœ¨æ›œæ—¥', 'é‡‘æ›œæ—¥', 'åœŸæ›œæ—¥', 'æ—¥æ›œæ—¥']
    peak_weekday = weekday_names_full[weekday_data.index(max(weekday_data))]

    # å¤–å›½èªæ¯”ç‡
    foreign_count = sum(int(language_counts.get(l, 0)) for l in ["è‹±èª", "ä¸­å›½èª", "éŸ“å›½èª"])
    foreign_pct = foreign_count / total_messages * 100 if total_messages > 0 else 0

    # æœ€å¤šã‚«ãƒ†ã‚´ãƒª
    top_cat = category_data[0] if category_data else None
    top_cat_name = top_cat["name"] if top_cat else ""
    top_cat_pct = top_cat["percent"] if top_cat else 0
    sorted_cats = sorted(category_data, key=lambda x: x["count"], reverse=True)
    top_cat_name = sorted_cats[0]["name"] if sorted_cats else ""
    top_cat_pct = sorted_cats[0]["percent"] if sorted_cats else 0

    # æœªå›ç­”ã®ä¸»è¦ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—
    total_errors = error_stats["info_nashi"] + error_stats["search_fail"] + error_stats["reconfirm"]
    main_error_pct = error_stats["info_nashi"] / total_errors * 100 if total_errors > 0 else 0

    summary_items_ja = [
        f"<strong>åˆ©ç”¨çŠ¶æ³</strong>ï¼šæœˆé–“<span class='highlight-text'>{total_messages}ä»¶</span>ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã€æ­£å¸¸å›ç­”ç‡ã¯{kpi['normal_answer_rate']:.1f}%",
        f"<strong>è³ªå•å‚¾å‘</strong>ï¼šæœ€å¤šã‚«ãƒ†ã‚´ãƒªã¯<span class='highlight-text'>ã€Œ{top_cat_name}ã€ï¼ˆ{top_cat_pct:.0f}%ï¼‰</span>",
        f"<strong>åˆ©ç”¨ãƒ”ãƒ¼ã‚¯</strong>ï¼š<span class='highlight-text'>{peak_hour_label}ï¼ˆ{peak_hour_pct:.0f}%ï¼‰</span>ã«é›†ä¸­ã€{peak_weekday}ãŒæœ€å¤š",
        f"<strong>ã‚¤ãƒ³ãƒã‚¦ãƒ³ãƒ‰</strong>ï¼š<span class='highlight-text'>ç´„{foreign_pct:.0f}%ãŒå¤–å›½èª</span>ã§ã®å•ã„åˆã‚ã›",
        f"<strong>æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ</strong>ï¼šæœªå›ç­”ã®<span class='highlight-text'>{main_error_pct:.0f}%ã¯æƒ…å ±ãªã—ã‚¨ãƒ©ãƒ¼</span>ã§FAQæ‹¡å……ã§æ”¹å–„å¯èƒ½",
    ]
    summary_items_zh = [
        f"<strong>ä½¿ç”¨æƒ…å†µ</strong>ï¼šæœˆåº¦å¤„ç†<span class='highlight-text'>{total_messages}æ¡</span>æ¶ˆæ¯ï¼Œæ­£å¸¸å›ç­”ç‡{kpi['normal_answer_rate']:.1f}%",
        f"<strong>é—®é¢˜è¶‹åŠ¿</strong>ï¼šæœ€å¤šç±»åˆ«ä¸º<span class='highlight-text'>ã€Œ{top_cat_name}ã€ï¼ˆ{top_cat_pct:.0f}%ï¼‰</span>",
        f"<strong>ä½¿ç”¨é«˜å³°</strong>ï¼šé›†ä¸­åœ¨<span class='highlight-text'>{peak_hour_label}ï¼ˆ{peak_hour_pct:.0f}%ï¼‰</span>ï¼Œ{peak_weekday}æœ€å¤š",
        f"<strong>å…¥å¢ƒæ¸¸</strong>ï¼š<span class='highlight-text'>çº¦{foreign_pct:.0f}%ä¸ºå¤–è¯­</span>å’¨è¯¢",
        f"<strong>æ”¹å–„è¦ç‚¹</strong>ï¼š<span class='highlight-text'>{main_error_pct:.0f}%ä¸ºä¿¡æ¯ç¼ºå¤±é”™è¯¯</span>ï¼Œå¯é€šè¿‡å®Œå–„FAQæ”¹å–„",
    ]

    summary_html = "\n".join(
        f'                        <li data-ja="{ja}" data-zh="{zh}">{ja}</li>'
        for ja, zh in zip(summary_items_ja, summary_items_zh)
    )
    # åˆ†æã‚µãƒãƒªãƒ¼ã®ulå†…å®¹ã‚’ç½®æ›
    main_html = re.sub(
        r'(<div class="summary-content">\s*<ul>).*?(</ul>)',
        rf'\1\n{summary_html}\n                    \2',
        main_html,
        flags=re.DOTALL,
    )

    # ========================================
    # æ”¹å–„ææ¡ˆï¼ˆå‹•çš„ç”Ÿæˆï¼‰
    # ========================================

    # æœªå›ç­”è³ªå•ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦åˆ†æã§ææ¡ˆã‚’ç”Ÿæˆ
    suggestion_items = []

    # ææ¡ˆ1: æƒ…å ±ãªã—ã‚¨ãƒ©ãƒ¼ãŒå¤šã„å ´åˆ
    if error_stats["info_nashi"] > 0:
        # æœªå›ç­”è³ªå•ã‹ã‚‰é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        unanswered_questions = unanswered_df['è³ªå•'].tolist()
        all_words = []
        for q in unanswered_questions:
            q_str = str(q)
            # ç°¡æ˜“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆã‚«ã‚¿ã‚«ãƒŠèªã€æ¼¢å­—èªï¼‰
            words = re.findall(r'[\u30A0-\u30FF]{2,}|[\u4E00-\u9FFF]{2,}', q_str)
            all_words.extend(words)
        word_freq = Counter(all_words).most_common(5)
        top_words = 'ã€ã€Œ'.join([w for w, c in word_freq[:3]]) if word_freq else 'ä¸æ˜'

        suggestion_items.append({
            "priority": "high", "badge": "badge-red",
            "title_ja": "FAQæƒ…å ±ã®æ‹¡å……", "title_zh": "FAQä¿¡æ¯è¡¥å……",
            "desc_ja": f"ã€Œ{top_words}ã€ãªã©æœªå›ç­”ã¨ãªã£ã¦ã„ã‚‹è³ªå•ã®FAQè¿½åŠ ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ï¼ˆæƒ…å ±ãªã—: {error_stats['info_nashi']}ä»¶ï¼‰",
            "desc_zh": f"å»ºè®®è¡¥å……ã€Œ{top_words}ã€ç­‰æœªå›ç­”é—®é¢˜çš„FAQã€‚ï¼ˆä¿¡æ¯ç¼ºå¤±: {error_stats['info_nashi']}ä»¶ï¼‰",
        })

    # ææ¡ˆ2: æ¤œç´¢å¤±æ•—ãŒå¤šã„å ´åˆ
    if error_stats["search_fail"] > 5:
        suggestion_items.append({
            "priority": "high", "badge": "badge-red",
            "title_ja": "ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ç²¾åº¦å‘ä¸Š", "title_zh": "çŸ¥è¯†åº“æœç´¢ç²¾åº¦æå‡",
            "desc_ja": f"æ¤œç´¢å¤±æ•—ãŒ{error_stats['search_fail']}ä»¶ã‚ã‚Šã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®æ§‹é€ ã‚„è¡¨è¨˜ã‚†ã‚Œã¸ã®å¯¾å¿œæ”¹å–„ã‚’æ¨å¥¨ã—ã¾ã™ã€‚",
            "desc_zh": f"æœç´¢å¤±è´¥{error_stats['search_fail']}ä»¶ï¼Œå»ºè®®æ”¹å–„çŸ¥è¯†åº“ç»“æ„å’ŒåŒä¹‰è¯å¤„ç†ã€‚",
        })

    # ææ¡ˆ3: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç‡ãŒä½ã„å ´åˆ
    if kpi["feedback_rate"] < 5:
        suggestion_items.append({
            "priority": "medium", "badge": "badge-yellow",
            "title_ja": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ä¿ƒé€²", "title_zh": "ä¿ƒè¿›ç”¨æˆ·åé¦ˆ",
            "desc_ja": f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç‡ãŒ{kpi['feedback_rate']:.1f}%ã¨ä½ã„ãŸã‚ã€è©•ä¾¡ãƒœã‚¿ãƒ³ã®è¦–èªæ€§å‘ä¸Šã‚’æ¨å¥¨ã—ã¾ã™ã€‚",
            "desc_zh": f"åé¦ˆç‡ä»…{kpi['feedback_rate']:.1f}%ï¼Œå»ºè®®æé«˜è¯„ä»·æŒ‰é’®çš„å¯è§æ€§ã€‚",
        })

    # ææ¡ˆ4: å¤–å›½èªãŒå¤šã„å ´åˆ
    if foreign_pct > 5:
        top_foreign = max(["è‹±èª", "ä¸­å›½èª", "éŸ“å›½èª"], key=lambda l: int(language_counts.get(l, 0)))
        suggestion_items.append({
            "priority": "medium", "badge": "badge-yellow",
            "title_ja": f"å¤šè¨€èªå¯¾å¿œã®å¼·åŒ–ï¼ˆ{top_foreign}ï¼‰", "title_zh": f"åŠ å¼ºå¤šè¯­è¨€æ”¯æŒï¼ˆ{top_foreign}ï¼‰",
            "desc_ja": f"å¤–å›½èªã§ã®å•ã„åˆã‚ã›ãŒç´„{foreign_pct:.0f}%ã‚ã‚Šã€{top_foreign}ã®FAQå……å®Ÿã‚’æ¨å¥¨ã—ã¾ã™ã€‚",
            "desc_zh": f"å¤–è¯­å’¨è¯¢çº¦{foreign_pct:.0f}%ï¼Œå»ºè®®å……å®{top_foreign}FAQã€‚",
        })

    # ææ¡ˆ5: å¥½è©•ç‡ãŒä½ã„å ´åˆ
    if kpi["good_rate"] < 60:
        suggestion_items.append({
            "priority": "high", "badge": "badge-red",
            "title_ja": "å›ç­”å“è³ªã®æ”¹å–„", "title_zh": "å›ç­”è´¨é‡æ”¹å–„",
            "desc_ja": f"å¥½è©•ç‡ãŒ{kpi['good_rate']:.1f}%ã¨ä½ã„ãŸã‚ã€å›ç­”ã®è©³ç´°åº¦ã‚„æ­£ç¢ºæ€§ã®è¦‹ç›´ã—ã‚’æ¨å¥¨ã—ã¾ã™ã€‚",
            "desc_zh": f"å¥½è¯„ç‡{kpi['good_rate']:.1f}%åä½ï¼Œå»ºè®®æ”¹å–„å›ç­”çš„è¯¦ç»†åº¦å’Œå‡†ç¡®æ€§ã€‚",
        })

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ææ¡ˆãŒå°‘ãªã„å ´åˆ
    if len(suggestion_items) < 2:
        suggestion_items.append({
            "priority": "low", "badge": "badge-green",
            "title_ja": "å®šæœŸçš„ãªFAQè¦‹ç›´ã—", "title_zh": "å®šæœŸFAQå®¡æŸ¥",
            "desc_ja": "æœˆæ¬¡ã§ã®FAQè¦‹ç›´ã—ã¨æ›´æ–°ã‚’ç¶™ç¶šã—ã€å›ç­”å“è³ªã®ç¶­æŒå‘ä¸Šã‚’å›³ã‚Šã¾ã—ã‚‡ã†ã€‚",
            "desc_zh": "å»ºè®®æ¯æœˆå®¡æŸ¥å¹¶æ›´æ–°FAQï¼ŒæŒç»­æå‡å›ç­”è´¨é‡ã€‚",
        })

    suggestion_html = ""
    for item in suggestion_items:
        priority_class = f"priority-{item['priority']}"
        suggestion_html += f'''
                <li class="suggestion-item {priority_class}">
                    <div class="suggestion-title">
                        <span class="badge {item['badge']}" data-ja="å„ªå…ˆåº¦ï¼š{item['priority'].replace('high','é«˜').replace('medium','ä¸­').replace('low','ä½')}" data-zh="ä¼˜å…ˆçº§ï¼š{item['priority'].replace('high','é«˜').replace('medium','ä¸­').replace('low','ä½')}">å„ªå…ˆåº¦ï¼š{item['priority'].replace('high','é«˜').replace('medium','ä¸­').replace('low','ä½')}</span>
                        <span data-ja="{item['title_ja']}" data-zh="{item['title_zh']}">{item['title_ja']}</span>
                    </div>
                    <div class="suggestion-desc" data-ja="{item['desc_ja']}" data-zh="{item['desc_zh']}">{item['desc_ja']}</div>
                </li>'''

    # æ”¹å–„ææ¡ˆã®ulå†…å®¹ã‚’ç½®æ›
    main_html = re.sub(
        r'(<ul class="suggestion-list">).*?(</ul>)',
        rf'\1{suggestion_html}\n            \2',
        main_html,
        flags=re.DOTALL,
    )

    # ========================================
    # æœªå›ç­”ä¸€è¦§ãƒšãƒ¼ã‚¸ç”Ÿæˆ
    # ========================================

    total_unanswered = len(unanswered_list)
    info_nashi_pct = int(error_stats["info_nashi"] / total_unanswered * 100) if total_unanswered > 0 else 0
    search_fail_pct = int(error_stats["search_fail"] / total_unanswered * 100) if total_unanswered > 0 else 0
    reconfirm_pct = int(error_stats["reconfirm"] / total_unanswered * 100) if total_unanswered > 0 else 0

    unanswered_html = unanswered_template
    unanswered_html = unanswered_html.replace("{{CLIENT_NAME}}", client_name)
    unanswered_html = unanswered_html.replace("{{PERIOD}}", period)
    unanswered_html = unanswered_html.replace("{{MAIN_REPORT_FILENAME}}", f"{client_name}_{period}_åˆ†æãƒ¬ãƒãƒ¼ãƒˆ.html")
    unanswered_html = unanswered_html.replace("{{TOTAL_UNANSWERED}}", str(total_unanswered))
    unanswered_html = unanswered_html.replace("{{INFO_NASHI_COUNT}}", str(error_stats["info_nashi"]))
    unanswered_html = unanswered_html.replace("{{INFO_NASHI_PERCENT}}", str(info_nashi_pct))
    unanswered_html = unanswered_html.replace("{{SEARCH_FAIL_COUNT}}", str(error_stats["search_fail"]))
    unanswered_html = unanswered_html.replace("{{SEARCH_FAIL_PERCENT}}", str(search_fail_pct))
    unanswered_html = unanswered_html.replace("{{RECONFIRM_COUNT}}", str(error_stats["reconfirm"]))
    unanswered_html = unanswered_html.replace("{{RECONFIRM_PERCENT}}", str(reconfirm_pct))

    table_rows = ""
    for i, item in enumerate(unanswered_list, 1):
        table_rows += f'''
<tr>
    <td class="number">{i}</td>
    <td class="timestamp">{item['timestamp']}</td>
    <td><span class="badge {item['badge_class']}" data-ja="{item['error_type']}" data-zh="{item['error_type']}">{item['error_type']}</span></td>
    <td class="question-cell">{item['question']}</td>
    <td class="answer-cell">{item['answer']}</td>
</tr>'''
    unanswered_html = unanswered_html.replace("{{TABLE_ROWS}}", table_rows)

    # ========================================
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    # ========================================

    output_dir.mkdir(parents=True, exist_ok=True)

    main_filename = output_dir / f"{client_name}_{period}_åˆ†æãƒ¬ãƒãƒ¼ãƒˆ.html"
    with open(main_filename, "w", encoding="utf-8") as f:
        f.write(main_html)
    print(f"\nâœ… ãƒ¡ã‚¤ãƒ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {main_filename}")

    sub_filename = None
    if total_unanswered > 10:
        sub_filename = output_dir / f"{client_name}_{period}_æœªå›ç­”ä¸€è¦§.html"
        with open(sub_filename, "w", encoding="utf-8") as f:
            f.write(unanswered_html)
        print(f"âœ… æœªå›ç­”ä¸€è¦§ç”Ÿæˆ: {sub_filename}")

    # ========================================
    # ã‚µã‚¤ãƒˆçµ±åˆï¼ˆ--site-dir æŒ‡å®šæ™‚ï¼‰
    # ========================================

    if args.site_dir and args.client_slug:
        site_dir = Path(args.site_dir).resolve()

        # year_month ã‚’ period ã‹ã‚‰æŠ½å‡ºï¼ˆä¾‹: "2026å¹´1æœˆ" â†’ "2026-01"ï¼‰
        ym_match = re.search(r'(\d{4})\D+(\d{1,2})', period)
        if ym_match:
            year_month = f"{ym_match.group(1)}-{int(ym_match.group(2)):02d}"
        else:
            year_month = df['è³ªå•æ™‚é–“'].iloc[0].strftime('%Y-%m')

        # KPIçµ±è¨ˆã‚’è¨ˆç®—
        month_stats = compute_kpi_for_dashboard(
            df, kpi, period, year_month, avg_daily, language_counts, feedback_rate
        )

        # dashboard-data.json ã‚’æ›´æ–°
        update_dashboard_json(
            site_dir, args.client_slug, client_name, month_stats,
            str(main_filename),
            str(sub_filename) if total_unanswered > 10 else None,
        )

        # ãƒ¬ãƒãƒ¼ãƒˆHTMLã‚’ã‚µã‚¤ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
        reports_dir = site_dir / "clients" / args.client_slug / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)

        dest_main = reports_dir / f"{year_month}.html"
        shutil.copy2(main_filename, dest_main)
        # ã‚µã‚¤ãƒˆç”¨ã«ãƒªãƒ³ã‚¯ã‚’ä¿®æ­£ï¼ˆæœªå›ç­”ä¸€è¦§ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç›¸å¯¾ãƒ‘ã‚¹ã«ï¼‰
        with open(dest_main, "r", encoding="utf-8") as f:
            html_content = f.read()
        html_content = html_content.replace(
            f'href="{client_name}_{period}_æœªå›ç­”ä¸€è¦§.html"',
            f'href="{year_month}_unanswered.html"'
        )
        with open(dest_main, "w", encoding="utf-8") as f:
            f.write(html_content)
        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆã‚³ãƒ”ãƒ¼: {dest_main}")

        if total_unanswered > 10:
            dest_sub = reports_dir / f"{year_month}_unanswered.html"
            shutil.copy2(sub_filename, dest_sub)
            print(f"âœ… æœªå›ç­”ä¸€è¦§ã‚³ãƒ”ãƒ¼: {dest_sub}")

    print("\nğŸ‰ åˆ†æå®Œäº†ï¼")


if __name__ == "__main__":
    main()
